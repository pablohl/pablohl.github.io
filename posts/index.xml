<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
	<channel>
		<title>Posts on Pablo Hernandez Leal</title>
		<link>https://pablohl.github.io/posts/</link>
		<description>Recent content in Posts on Pablo Hernandez Leal</description>
		<generator>Hugo -- gohugo.io</generator>
		<language>en-us</language>
		<copyright>This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.</copyright>
		<lastBuildDate>Wed, 12 Jun 2024 00:00:00 +0000</lastBuildDate>
		<atom:link href="https://pablohl.github.io/posts/index.xml" rel="self" type="application/rss+xml" />
		
		<item>
			<title>Prompting, the Wonderful Wizard (Part 1)</title>
			<link>https://pablohl.github.io/posts/prompts/</link>
			<pubDate>Wed, 12 Jun 2024 00:00:00 +0000</pubDate>
			
			<guid>https://pablohl.github.io/posts/prompts/</guid>
			<description>This is my first post about Large Language Models (LLMs), and given all the information available, it&amp;rsquo;s challenging to write something novel that won&amp;rsquo;t become outdated soon. I wanted to start with something &amp;ldquo;reasonably&amp;rdquo; simple: prompting. In hindsight, I&amp;rsquo;ll conclude my post by arguing that prompting is not simple. It is both underestimated and overestimated in different aspects. In this first part of the post, my goal is to answer the following questions:</description>
			<content type="html"><![CDATA[<hr>
<p>This is my first post about Large Language Models (LLMs), and given all the information available, it&rsquo;s challenging to write something novel that won&rsquo;t become outdated soon. I wanted to start with something &ldquo;reasonably&rdquo; simple: prompting. In hindsight, I&rsquo;ll conclude my post by arguing that prompting is not simple. It is both underestimated and overestimated in different aspects. In this first part of the post, my goal is to answer the following questions:</p>
<ul>
<li><a href="#how-it-started">How it started?</a>
<ul>
<li><a href="#in-context-learning">In-context-learning</a></li>
</ul>
</li>
<li><a href="#how-has-it-evolved-and-why-is-it-important">How has it evolved, and why is it important?</a></li>
<li><a href="#how-does-it-work">How does it work?</a>
<ul>
<li><a href="#idea-1-functional-approach">Functional approach</a></li>
<li><a href="#idea-2-in-context-learning-as-a-known-learning-process">Cast as a known process</a></li>
</ul>
</li>
<li><a href="#when-does-it-not-work">When does it not work?</a>
<ul>
<li><a href="#example-1-planning">Planning</a></li>
<li><a href="#example-2-compositional-tasks">Compositional tasks</a></li>
</ul>
</li>
</ul>
<p>Note: I&rsquo;ll try not to overuse jargon and keep things simple.</p>
<p><img src="../../img/termstweet.png" alt="Prompting or in context learning"></p>
<hr>
<h1 id="how-it-started">How It Started?</h1>
<pre><code>  “What is he like?” asked the girl.
  “That is hard to tell,” said the man thoughtfully.
  “You see, Oz is a Great Wizard, and can take on any form he wishes.
  So that some say he looks like a bird;
  and some say he looks like an elephant;
  and some say he looks like a cat.
  To others he appears as a beautiful fairy,
  or a brownie, or in any other form that pleases him.
  But who the real Oz is,
  when he is in his own form,
  no living person can tell.”
                                      - The Wonderful Wizard of Oz
</code></pre>
<p>Reading the abstract of the GPT-3 paper<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup> (from 2020) now feels like reading a paper that was written decades ago, since everything seems so familiar and no longer new.</p>
<blockquote>
<p>We demonstrate that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even becoming competitive with prior state-ofthe-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous nonsparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks
and few-shot demonstrations specified purely via text interaction with the model.
GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks.<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup></p>
</blockquote>
<p>Let&rsquo;s highlight two key points from that paragraph:</p>
<ul>
<li>
<ol>
<li>The scale of the model (175 billion parameters).</li>
</ol>
</li>
<li>
<ol start="2">
<li>The proposal of solving many tasks with no additional fine-tunning, instead only using few-shot demonstrations (i.e., in-context learning) on a <em>frozen</em> LLM.</li>
</ol>
</li>
</ul>
<blockquote>
<p>the model is given a few demonstrations of the task at inference time as conditioning, but no weights are updated. An example typically has a context and a desired completion (for example an English sentence and the French translation), and few-shot works by giving K examples of context and completion, and then one final example of context, with the model expected to provide the completion. We typically set K in the range of 10 to 100, as this is how many examples can
fit in the model’s context window (nctx = 2048). The main advantage of few-shot is a major reduction in the need for task-specific data.<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup></p>
</blockquote>
<h2 id="in-context-learning">In-context learning</h2>
<p><img src="../../img/icl2.png" alt="Prompting or in context learning">
<em>Example of in context-learning, source<sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup></em></p>
<p>In simple terms, the input to the LLM needs some context and examples of the task to solve, all in natural language, and <strong>that’s it</strong>. If there are no examples, it’s called zero-shot. If there’s a single example, it’s called one-shot. You get the idea.</p>
<p>I remember when I learned about this concept of “prompting.” My initial reaction was disbelief at its power. After all, how could it be possible that just with some instructions and context it could solve complex tasks?</p>
<h1 id="how-has-it-evolved-and-why-is-it-important">How Has It Evolved, and Why Is It Important?</h1>
<p>Naturally, the big claims of GPT-3 were further tested by researchers. Two main questions were highly relevant:</p>
<ul>
<li>What tasks can in-context learning solve? (testing the limits)</li>
<li>How does in-context learning work? (improving our understanding)</li>
</ul>
<p>Regarding improving our understanding of in-context learning, a first attempt defined the <em>emergent abilities</em> of LLMs:</p>
<blockquote>
<p>The ability to perform a task via few-shot prompting is emergent when a model has random performance until a certain scale, after which performance increases to well-above random.<sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup></p>
</blockquote>
<blockquote>
<p>Emergent abilities would not have been directly predicted by extrapolating a scaling law (i.e. consistent performance improvements) from small-scale models. When visualized via a scaling curve (x-axis: model scale, y-axis: performance), emergent abilities show a clear pattern—performance is near-random until a certain critical threshold of scale is reached, after which performance increases to substantially above random. This qualitative change is also known as a phase transition—a dramatic change in overall behavior that would not have been foreseen by examining smaller-scale systems.<sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup></p>
</blockquote>
<p>So, again we see that <em>scale</em> plays a key role in the success of in-context learning.</p>
<p>Regarding testing the limits of in-context learning, the paper evaluated 10 abilities/tasks that LLMs were able to achieve, such as question answering, conceptual mapping, and simple arithmetic.</p>
<p>At this point, our understanding of prompting was (and perhaps still is) not very good. We know that some abilities emerge in these LLMs, and those are quite powerful because they can solve reasonably hard tasks. In-context learning seems quite simple, so researchers continued pushing with more elaborate prompts.</p>
<p>A notable proposal is the idea of multi-step reasoning, where a common example is the <strong>chain-of-thought prompting</strong>, as exemplified below:
<img src="../../img/cot2.png" alt="Prompting or in context learning">
<em>Example of in Chain of thought prompt, source<sup id="fnref:4"><a href="#fn:4" class="footnote-ref" role="doc-noteref">4</a></sup></em></p>
<p>The same paper studied 13 abilities that emerged with &ldquo;augmented prompts.&rdquo;</p>
<p><img src="../../img/emergentabilities.png" alt="ResarchML">
<em>Emergent abilities of LLMs, source<sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup></em></p>
<p>In summary, prompting has shown a broad range of applicability (many tasks) and flexibility (demonstrations vary from 1 to hundreds, it can adapt to different domains, it can be augmented, etc.).</p>
<p>Now, let&rsquo;s try to answer the question, <strong>how?</strong></p>
<h1 id="how-does-it-work">How Does It Work?</h1>
<p>Short answer: we don&rsquo;t know yet.</p>
<p>Long answer: researchers have tried different ideas to understand in-context learning.</p>
<h2 id="idea-1-functional-approach">Idea #1 Functional Approach</h2>
<p>Researchers have studied what kind of functions LLMs/transformers can learn.</p>
<ul>
<li>Example #1.1 Learning Linear Functions</li>
</ul>
<blockquote>
<p>The building blocks for two specific procedures—gradient descent on the least-squares objective and closed-form computation of its minimizer—are implementable by transformer networks. These constructions show that, in principle, fixed transformer parameterizations are expressive enough to simulate these learning algorithms.<sup id="fnref:5"><a href="#fn:5" class="footnote-ref" role="doc-noteref">5</a></sup></p>
</blockquote>
<blockquote>
<p>We showed that these models are capable in theory of implementing multiple linear regression algorithms, that they empirically implement this range of algorithms (transitioning between algorithms depending on model capacity and dataset noise), and finally that they can be probed for intermediate quantities computed by these algorithms.<sup id="fnref:5"><a href="#fn:5" class="footnote-ref" role="doc-noteref">5</a></sup></p>
</blockquote>
<ul>
<li>Example #1.2 Learning Non-Linear Functions</li>
</ul>
<blockquote>
<p>In this work, we formalize and study the question: can we train models that learn different classes of functions in-context? We show that Transformer models trained from scratch can in-context learn the class of linear functions, with performance comparable to the optimal least squares estimator,
even under distribution shifts. Moreover, we show that in-context learning is also possible for sparse
linear functions, decision trees, and two-layer neural networks; learning problems which are solved
in practice with involved iterative algorithms such as gradient descent.<sup id="fnref:6"><a href="#fn:6" class="footnote-ref" role="doc-noteref">6</a></sup></p>
</blockquote>
<p>These findings suggest that Transformers can learn various classes of functions, not just linear, but also more complex ones like decision trees and two-layer neural networks.</p>
<h2 id="idea-2-in-context-learning-as-a-known-learning-process">Idea #2 In-Context Learning as a Known Learning Process</h2>
<ul>
<li>Example #2.1 In-context learning as gradient descent.</li>
</ul>
<p>This work is quite detailed and structured; it is hard to summarize, so I just decided to highlight one of the many experiments that they did.</p>
<blockquote>
<p>How does ICL evolve during training?
From the given constructions, models need to arrive at very specific weights
to be able to perform gradient descent on in-context samples, but in practice, we observe models develop, retain, and
improve this ability over time in training when the parameters change significantly.<sup id="fnref:7"><a href="#fn:7" class="footnote-ref" role="doc-noteref">7</a></sup></p>
</blockquote>
<p><img src="../../img/icltraining.png" alt="ResarchML">
<em>In context learning accuracy during training, source <sup id="fnref:7"><a href="#fn:7" class="footnote-ref" role="doc-noteref">7</a></sup></em></p>
<p>This experiment evaluated the accuracy of in-context learning during training, showing that the ability is quite stable even though the parameters are not.</p>
<ul>
<li>Example #2.2 In-context learning as Bayesian inference</li>
</ul>
<blockquote>
<p>We cast in-context learning as implicit Bayesian inference, where the pretrained LM implicitly infers a concept when making a prediction. We show that in-context learning occurs when the pretraining distribution is a mixture of HMMs.<sup id="fnref:8"><a href="#fn:8" class="footnote-ref" role="doc-noteref">8</a></sup></p>
</blockquote>
<p><img src="../../img/bayes2.png" alt="ResarchML">
<em>In context learning as Bayesian inference, source <sup id="fnref:8"><a href="#fn:8" class="footnote-ref" role="doc-noteref">8</a></sup></em></p>
<p>At this point, we do not have a definite answer on how ICL works.</p>
<blockquote>
<p>While recent work has shown that Transformers have the expressive capacity to simulate gradient-descent in their forward pass, this does not immediately imply that real-world models actually do simulate it.<sup id="fnref:7"><a href="#fn:7" class="footnote-ref" role="doc-noteref">7</a></sup></p>
</blockquote>
<h1 id="when-does-it-not-work">When Does It Not Work?</h1>
<p>It&rsquo;s safe to say that the general consensus is that LLMs are powerful and can solve diverse tasks in many domains.</p>
<blockquote>
<p>We demonstrate that, beyond its mastery of language, GPT-4 can solve novel and difficult tasks that span mathematics, coding, vision, medicine, law, psychology and more, without needing any special prompting<sup id="fnref:9"><a href="#fn:9" class="footnote-ref" role="doc-noteref">9</a></sup>.</p>
</blockquote>
<p><img src="../../img/sparks0.png" alt="ResarchML">
<img src="../../img/sparks1.png" alt="ResarchML">
<em>Table of contents of &ldquo;Sparks of Artificial General Intelligence: Early experiments with GPT-4&rdquo; <sup id="fnref:9"><a href="#fn:9" class="footnote-ref" role="doc-noteref">9</a></sup></em></p>
<p>However, prompting/LLMs have limitations. As the field has rapidly evolved, new research has shown how brittle LLMs can be in certain contexts and applications.</p>
<h2 id="example-1-planning">Example #1: Planning</h2>
<p><img src="../../img/roguegpt4.png" alt="ResarchML"></p>
<blockquote>
<p>We conduct a systematic study by generating a suite of instances on
domains similar to the ones employed in the International Planning Competition.<sup id="fnref:10"><a href="#fn:10" class="footnote-ref" role="doc-noteref">10</a></sup></p>
</blockquote>
<blockquote>
<p>Our results show that even in simple common-sense planning domains
where humans could easily come up with plans, LLMs like GPT-3 exhibit a dismal performance.
Even though there is an uptick in the performance by the newer GPT-4 in the blocksworld domain,
it still fails miserably on the mystery blocksworld domain, indicating their inability to reason in an
abstract manner.<sup id="fnref:10"><a href="#fn:10" class="footnote-ref" role="doc-noteref">10</a></sup></p>
</blockquote>
<p>But, you might be asking, what about those &ldquo;augmented prompting&rdquo; techniques? There are recent works that
have shown not-so-good perfomance with <a href="https://arxiv.org/abs/2405.04776">chain-of-thought</a><sup id="fnref:11"><a href="#fn:11" class="footnote-ref" role="doc-noteref">11</a></sup>, <a href="https://arxiv.org/abs/2402.08115">self-reflexivity</a><sup id="fnref:12"><a href="#fn:12" class="footnote-ref" role="doc-noteref">12</a></sup>, and <a href="https://arxiv.org/abs/2405.13966">reasoning and acting (ReAct)</a><sup id="fnref:13"><a href="#fn:13" class="footnote-ref" role="doc-noteref">13</a></sup>.</p>
<p><img src="../../img/cotplan.png" alt="ResarchML">
<em>Chain-of-thought accuracy in planning tasks, source <sup id="fnref:13"><a href="#fn:13" class="footnote-ref" role="doc-noteref">13</a></sup></em></p>
<blockquote>
<p>While the preceding discussion establishes that LLMs are not capable of generating correct plans in
autonomous mode, there is still the possibility that they can be useful in an LLM-Modulo mode as
idea generators for other sound external planners, verifiers or even humans-in-the-loop.<sup id="fnref:10"><a href="#fn:10" class="footnote-ref" role="doc-noteref">10</a></sup></p>
</blockquote>
<p><img src="../../img/modulo2.png" alt="ResarchML">
<em>LLM-Modulo, source <sup id="fnref:14"><a href="#fn:14" class="footnote-ref" role="doc-noteref">14</a></sup></em></p>
<p>If you are interested in this area, definitely check <a href="https://www.youtube.com/watch?v=hGXhFa3gzBs">this recent talk: Can LLMs Reason &amp; Plan?</a></p>
<h2 id="example-2-compositional-tasks">Example #2: Compositional Tasks</h2>
<blockquote>
<p>Transformers today demonstrate undeniably powerful empirical results. Yet, our study suggests that they may have fundamental
weaknesses in certain intellectual tasks that require true multi-step compositional operations such as
multiplications and logic puzzles.<sup id="fnref:15"><a href="#fn:15" class="footnote-ref" role="doc-noteref">15</a></sup></p>
</blockquote>
<blockquote>
<p>The proofs presented show that, under reasonable assumptions, the probability of incorrect predictions converges exponentially to
≈ 1 for abstract compositional tasks. Importantly, these proofs apply to autoregressive LMs in general. Our insights indicate that the current configuration of transformers, with their reliance on a greedy process for predicting the next word, constrains their error recovery capability and
impedes the development of a comprehensive global understanding of the task.<sup id="fnref:15"><a href="#fn:15" class="footnote-ref" role="doc-noteref">15</a></sup></p>
</blockquote>
<p>LLMs (at least the ones we have now) are not the AGI that can be used indiscriminately in any application. It is not a plug-and-play technology. LLMs are useful, and there are many applications, but it is clear that understanding more about their limits will only improve their success.</p>
<blockquote>
<p>To put it another way, the problem with Alchemy of yore is not that Chemistry is useless, but that people wanted to delude themselves that Chemistry –a pretty amazing discipline on its own merits– can be Nuclear Physics if you prompt it just so. The confusions regarding LLM abilities, or should we say, LLM alchemy, seems to be not that much different–oscillating between ignoring what they are good at, and ascribing them abilities they don’t have.<sup id="fnref:14"><a href="#fn:14" class="footnote-ref" role="doc-noteref">14</a></sup></p>
</blockquote>
<p><img src="../../img/encyplopedia.png" alt="ResarchML"></p>
<hr>
<p>Cite as:</p>
<blockquote>
<p>Hernandez-Leal, Pablo. (June 2024). Prompting, the Wonderful Wizard (Part 1), pablohl.github.io, <a href="https://pablohl.github.io/posts/prompts/">https://pablohl.github.io/posts/prompts/</a></p>
</blockquote>
<pre><code>  @article{hernandezleal2024prompting,
    title   = &quot;Prompting, the Wonderful Wizard (Part 1)&quot;,
    author  = &quot;Hernandez-Leal, Pablo&quot;,
    journal = &quot;pablohl.github.io&quot;,
    year    = &quot;2024&quot;,
    month   = &quot;June&quot;,
    url     = &quot;https://pablohl.github.io/posts/prompts/&quot;
  }
</code></pre>
<hr>
<section class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1" role="doc-endnote">
<p><a href="https://proceedings.neurips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf">Brown, Tom, et al. &ldquo;Language models are few-shot learners.&rdquo; (2020)</a>&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2" role="doc-endnote">
<p><a href="https://arxiv.org/pdf/2301.00234">Dong, Qingxiu, et al. &ldquo;A survey on in-context learning.&rdquo; (2022).</a>&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3" role="doc-endnote">
<p><a href="https://arxiv.org/pdf/2206.07682">Wei, Jason, et al. &ldquo;Emergent abilities of large language models.&rdquo; (2022).</a>&#160;<a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:4" role="doc-endnote">
<p><a href="https://proceedings.neurips.cc/paper_files/paper/2022/file/9d5609613524ecf4f15af0f7b31abca4-Paper-Conference.pdf">Wei, Jason, et al. &ldquo;Chain-of-thought prompting elicits reasoning in large language models.&rdquo; (2022)</a>&#160;<a href="#fnref:4" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:5" role="doc-endnote">
<p><a href="https://arxiv.org/pdf/2211.15661">Akyürek, Ekin, et al. &ldquo;What learning algorithm is in-context learning? investigations with linear models.&rdquo; (2022).</a>&#160;<a href="#fnref:5" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:6" role="doc-endnote">
<p><a href="https://proceedings.neurips.cc/paper_files/paper/2022/file/c529dba08a146ea8d6cf715ae8930cbe-Paper-Conference.pdf">Garg, Shivam, et al. &ldquo;What can transformers learn in-context? a case study of simple function classes.&rdquo; (2022)</a>&#160;<a href="#fnref:6" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:7" role="doc-endnote">
<p><a href="https://arxiv.org/pdf/2310.08540">Shen, Lingfeng, Aayush Mishra, and Daniel Khashabi. &ldquo;Do pretrained Transformers Really Learn In-context by Gradient Descent?.&rdquo; (2023).</a>&#160;<a href="#fnref:7" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:8" role="doc-endnote">
<p><a href="https://arxiv.org/pdf/2111.02080">Xie, Sang Michael, et al. &ldquo;An explanation of in-context learning as implicit bayesian inference.&rdquo; (2021).</a>&#160;<a href="#fnref:8" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:9" role="doc-endnote">
<p><a href="https://arxiv.org/pdf/2303.12712">Bubeck et al. Sparks of Artificial General Intelligence: Early experiments with GPT-4</a>&#160;<a href="#fnref:9" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:10" role="doc-endnote">
<p><a href="https://proceedings.neurips.cc/paper_files/paper/2023/hash/efb2072a358cefb75886a315a6fcf880-Abstract-Conference.html">Valmeekam, Karthik, et al. &ldquo;On the planning abilities of large language models-a critical investigation.&quot;(2023).</a>&#160;<a href="#fnref:10" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:11" role="doc-endnote">
<p><a href="https://arxiv.org/abs/2405.04776">Stechly, Kaya, Karthik Valmeekam, and Subbarao Kambhampati. &ldquo;Chain of Thoughtlessness: An Analysis of CoT in Planning.&rdquo; (2024).</a>&#160;<a href="#fnref:11" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:12" role="doc-endnote">
<p><a href="https://arxiv.org/abs/2402.08115">Stechly, Kaya, Karthik Valmeekam, and Subbarao Kambhampati. &ldquo;On the Self-Verification Limitations of Large Language Models on Reasoning and Planning Tasks.&rdquo; (2024).</a>&#160;<a href="#fnref:12" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:13" role="doc-endnote">
<p><a href="https://arxiv.org/abs/2405.13966">Verma, Mudit, Siddhant Bhambri, and Subbarao Kambhampati. &ldquo;On the Brittle Foundations of ReAct Prompting for Agentic Large Language Models.&rdquo; (2024).</a>&#160;<a href="#fnref:13" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:14" role="doc-endnote">
<p><a href="https://arxiv.org/pdf/2402.01817">Kambhampati, Subbarao, et al. &ldquo;LLMs Can&rsquo;t Plan, But Can Help Planning in LLM-Modulo Frameworks.&rdquo; (2024).</a>&#160;<a href="#fnref:14" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:15" role="doc-endnote">
<p><a href="https://proceedings.neurips.cc/paper_files/paper/2023/file/deb3c28192f979302c157cb653c15e90-Paper-Conference.pdf">Dziri, Nouha, et al. &ldquo;Faith and fate: Limits of transformers on compositionality.&rdquo; (2024).</a>&#160;<a href="#fnref:15" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</section>
]]></content>
		</item>
		
		<item>
			<title>Lessons From Six Years of Applied Machine Learning in Industry</title>
			<link>https://pablohl.github.io/posts/industryvsacademia/</link>
			<pubDate>Tue, 04 Jun 2024 00:00:00 +0000</pubDate>
			
			<guid>https://pablohl.github.io/posts/industryvsacademia/</guid>
			<description>It’s been more than six years since I started working in industry doing Machine Learning. I wanted to delve a bit deeper into my experience and what I have seen as differences between academic research (PhD) and applied research/data science. Below, I describe six lessons I have learned.
 1. I don’t have the time to do all the experiments I would like. When trying to get your research published, there are many experiments you need to perform.</description>
			<content type="html"><![CDATA[<hr>
<p>It’s been more than six years since I started working in industry doing Machine Learning. I wanted to delve a bit deeper into my experience and what I have seen as differences between academic research (PhD) and applied research/data science. Below, I describe six lessons I have learned.</p>
<hr>
<h2 id="1-i-dont-have-the-time-to-do-all-the-experiments-i-would-like">1. I don’t have the time to do all the experiments I would like.</h2>
<p>When trying to get your research published, there are many experiments you need to perform. Experiments become nice plots and tables. You cannot have missing cells in your tables, and you cannot have missing curves in your plots. You need to be thorough and complete. That’s good for science and rigorous peer-review evaluation.</p>
<p>In industry, especially if you want to move quickly, you cannot do all those experiments. It is OK to stop your experiments before having all the cells in the table. It is OK if you don’t have all the points in your curve.</p>
<blockquote>
<p><a href="https://www.quora.com/What-is-the-origin-of-the-40-70-rule-stating-you-should-aim-to-have-at-least-40-of-the-info-required-to-make-a-decision-but-not-to-exceed-70-as-you-may-have-waited-too-long-This-is-often-attributed-to-Colin-Powell">40-70 rule: The rule states that you need between 40 and 70 per cent of the total information to make a decision. With less than 40 per cent, you will likely make a poor choice, and with more than 70 per cent, you will end up taking too long, and the decision will be made for you!</a></p>
</blockquote>
<p>The first reason to not need a complete evaluation is that the experiment has failed. It is clear that your hypothesis/idea did not work. You will be wasting time waiting for another seed/experiment/curve.</p>
<p>There is also a second moment where I sometimes would like to keep doing experiments: the results are good, they are better than the baseline, my manager/team is convinced this is a step forward. But there are still more ideas to try and more improvements possible.</p>
<blockquote>
<p>And let me tell you something, particularly young people here. Better is good. I used to have to tell my young staff this all time in the White House. Better is good. That’s the history of progress in this country — not perfect, better &ndash; Barack Obama.</p>
</blockquote>
<p>The second reason to stop experiments might be because your work is good enough to go into production. Research stops for now (it will come back, don’t worry).</p>
<h2 id="2-i-do-explorationexploitation-experiments">2. I do exploration/exploitation experiments.</h2>
<p>When doing academic research, you are usually part of a very small circle of expertise and you have a reasonably defined project. Your goal is to advance the state of the art, which limits your degrees of freedom on the things you can try.</p>
<p>When dealing with an industry problem, the tool/algorithm you use is completely irrelevant for MANY people (even your manager), as long as you solve it. That’s your job, solving a problem, not advancing the state of the art. Now I see it as an advantage, but it can also be daunting. What to try? Where to start?</p>
<p>There are problems in industry that are too complex to solve entirely. There are problems where there is no baseline to start with. There are problems where improvements can be made in 10 different parts of the process. This is where I try diverse ideas/experiments with sufficient diligence (exploration experiments). Maybe putting together an ensemble of classifiers instead of the one we are using might improve the performance, or maybe trying a completely new classifier, or different features. The point is to have diversity and see where most of the gains could be made. Once an area of improvement is identified, experiments shift to an exploitation phase, delving deeper into details.</p>
<blockquote>
<p><a href="https://genius.com/Chamath-palihapitiya-how-we-put-facebook-on-the-path-to-1-billion-users-annotated">You know, look, we actually just looked at a lot of data, we measured a lot of stuff, we tested a lot of stuff, and we tried a lot of stuff. We had a culture fortunately of experimenting and trying at ton of stuff and we celebrated failure more than we really celebrated success.</a> &ndash; Chamath Palihapitiya</p>
</blockquote>
<h2 id="3-i-need-to-take-many-decisions-on-my-own">3. I need to take many decisions on my own.</h2>
<blockquote>
<p>If you accept that perfection can&rsquo;t be reached, and that everything you add has the risk of making things worse, then you can start making tradeoffs.
It&rsquo;s easy, if you have a problem, to think of the solution in isolation—to work as though you have infinite time and energy for that one solution. But you exist in the physical world, where there are constraints.<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup></p>
</blockquote>
<p>There are many decisions to make and there are always tradeoffs. I need to decide which hyperparameters to test, how to preprocess data, which baselines to try, etc. In most cases, I make brief assessment and analyze the tradeoffs. Usually <em>time</em> is one salient factor to consider. High-stakes decisions (one-way doors) need more analysis and usually more people involved.</p>
<blockquote>
<p>&ldquo;Some decisions are consequential and irreversible or nearly irreversible – one-way doors – and these
decisions must be made methodically, carefully, slowly, with great deliberation and consultation. If you walk through and don’t like what you see on the other side, you can’t get back to where you were before. We can call these Type 1 decisions. But most decisions aren’t like that – they are changeable, reversible – they’re two-way doors. If you’ve made a suboptimal Type 2 decision, you don’t have to live with the consequences for that long. You can reopen the door and go back through. Type 2 decisions can and should be made quickly by high judgment individuals or small groups&rdquo;<sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup></p>
</blockquote>
<h2 id="4-i-care-much-more-about-the-data-instead-of-just-the-model">4. I care much more about the data instead of just the model.</h2>
<p>In academic research, models are what matter. In industry, models are important, but everything before the models is usually much more important. Data is what matters. Understanding your data means understanding your problem and understanding your problem might provide clues on how to better approach it.</p>
<p>Looking at the data, spending time before trying the first baseline is particularly important.</p>
<blockquote>
<p>The first step to training a neural net is to not touch any neural net code at all and instead begin by thoroughly inspecting your data. This step is critical. I like to spend copious amount of time (measured in units of hours) scanning through thousands of examples, understanding their distribution and looking for patterns. Luckily, your brain is pretty good at this. One time I discovered that the data contained duplicate examples. Another time I found corrupted images/labels. I look for data imbalances and biases. I will typically also pay attention to my own process for classifying the data, which hints at the kinds of architectures we’ll eventually explore. As an example - are very local features enough or do we need global context? How much variation is there and what form does it take? What variation is spurious and could be preprocessed out? Does spatial position matter or do we want to average pool it out? How much does detail matter and how far could we afford to downsample the images? How noisy are the labels?<sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup></p>
</blockquote>
<p><em>Cleaning</em> and <em>learning</em> from your data can be one of the most impactful things you can do for your result. After doing this on different occasions, I’ve discovered it’s actually not as boring as it may seem. The lack of proper motivation might be the issue; in industry, the data is noisy, complex, entangled, and usually unstructured. This is “real” data, and it presents a worthwhile challenge.</p>
<h2 id="5-i-have-to-switch-priorities-quickly">5. I have to switch priorities quickly.</h2>
<p>If you work as an “Applied Scientist” then inherently there’s a connotation of going beyond academic papers. It means you have a tighter integration with the entire business, which means you are using company data and models. It is probable that you are part of a cross-functional team, meaning you interact with more people.</p>
<blockquote>
<p>Similar to data scientists, applied scientists convert business problems (e.g., increasing revenue) into solutions (e.g., increased customer acquisition? improved search or recommendations? pricing models?) To go from problem to production, they need know-how on: building data pipelines, experimentation and prototyping, training and deploying ML models, and basic software engineering and devops. Their deliverables include code for ML systems and documents on their design, methodology, and experiments.<sup id="fnref:4"><a href="#fn:4" class="footnote-ref" role="doc-noteref">4</a></sup></p>
</blockquote>
<p>Having this broader spectrum of reachability has a consequence that there’s a range of responsibilities that have changing priorities. When you are responsible for something in production and something is not working, that takes priority. Other times, pure research has the top priority. I’ve noticed that having proper documentation helps with this context switching, even when you are the only person working on that part of project.</p>
<h2 id="6-i-can-see-the-impact-of-my-work">6. I can see the impact of my work.</h2>
<p>Academic research often means publishing in conferences and journals. However, in ML, the usefulness and applicability of the research are often relegated to a single paragraph in the Introduction of the paper. Generally, the experiments are conducted in smaller, controlled settings rather than in real-world environments. While there are valid reasons for this approach, it is evident that many papers do not have immediate industry applications. In practice, a published paper usually marks the end of that particular piece of research.</p>
<p><img src="../../img/product.png" alt="ResarchML"></p>
<blockquote>
<p>Papers typically motivate their projects by appealing to the needs of the ML research community and
rarely mention potential societal benefits.<sup id="fnref:5"><a href="#fn:5" class="footnote-ref" role="doc-noteref">5</a></sup></p>
</blockquote>
<blockquote>
<p>Even when societal needs are mentioned as part of the justification of the project, the connection is loose.<sup id="fnref:5"><a href="#fn:5" class="footnote-ref" role="doc-noteref">5</a></sup></p>
</blockquote>
<blockquote>
<p>The cursory nature of the connection between societal needs and the content of the paper also manifests in the fact that the societal needs, or the applicability to the real world, is often only discussed in the beginning of the papers. From papers that mention applicability to the real world, the vast majority of mentions are in the Introduction section, and applicability is rarely engaged with afterwards. Papers tend to introduce the problem as useful for applications in object detection or text classification, for example, but rarely justify why an application is worth contributing to, or revisit how they particularly contribute to an application as their result.<sup id="fnref:5"><a href="#fn:5" class="footnote-ref" role="doc-noteref">5</a></sup></p>
</blockquote>
<p>Working in industry usually means your work is not going to get published, which might mean losing that proud moment of going to conferences and presenting your work. However, when you successfully deploy something into production, you receive a different type of personal recognition: you can confidently say to yourself, &ldquo;That thing is running live, and I built it.&rdquo;</p>
<hr>
<section class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1" role="doc-endnote">
<p><a href="https://zapier.com/blog/tradeoffs-at-work/">Justin Deal, &ldquo;There&rsquo;s no such thing as perfection—only tradeoffs&rdquo;</a>&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2" role="doc-endnote">
<p><a href="https://s2.q4cdn.com/299287126/files/doc_financials/annual/2015-Letter-to-Shareholders.PDF">Jeffrey P. Bezos, &ldquo;2015 Letter to Shareholders&rdquo;</a>&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3" role="doc-endnote">
<p><a href="https://karpathy.github.io/2019/04/25/recipe/">Andrej Karpathy, &ldquo;A Recipe for Training Neural Networks&rdquo;</a>&#160;<a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:4" role="doc-endnote">
<p><a href="https://eugeneyan.com/writing/data-science-roles/">Dan, Ziyou, &ldquo;Applied / Research Scientist, ML Engineer: What’s the Difference?&quot;</a>&#160;<a href="#fnref:4" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:5" role="doc-endnote">
<p><a href="https://arxiv.org/pdf/2106.15590">Birhane, Abeba, et al. &ldquo;The values encoded in machine learning research.&rdquo; Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency. 2022.</a>&#160;<a href="#fnref:5" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</section>
]]></content>
		</item>
		
		<item>
			<title>Quantitative Research and Machine Learning</title>
			<link>https://pablohl.github.io/posts/simplicity/</link>
			<pubDate>Sun, 26 May 2024 00:00:00 +0000</pubDate>
			
			<guid>https://pablohl.github.io/posts/simplicity/</guid>
			<description>I think of myself as a Machine Learning (ML) person with my most recent application domain being finance. However, I&amp;rsquo;ve worked alongside &amp;ldquo;Quants&amp;rdquo;.
 According to ChatGPT: &amp;ldquo;In finance, a &amp;ldquo;quant&amp;rdquo; (short for quantitative analyst) is a professional who uses mathematical models, statistical techniques, and computer programming to analyze financial markets and securities. Quants develop algorithms and trading strategies to identify profitable opportunities and manage risk. They often work for investment banks, hedge funds, and other financial institutions, focusing on areas such as derivatives pricing, risk management, and algorithmic trading.</description>
			<content type="html"><![CDATA[<p>I think of myself as a Machine Learning (ML) person with my most recent application domain being finance. However, I&rsquo;ve worked alongside &ldquo;Quants&rdquo;.</p>
<blockquote>
<p>According to ChatGPT: &ldquo;In finance, a &ldquo;quant&rdquo; (short for quantitative analyst) is a professional who uses mathematical models, statistical techniques, and computer programming to analyze financial markets and securities. Quants develop algorithms and trading strategies to identify profitable opportunities and manage risk. They often work for investment banks, hedge funds, and other financial institutions, focusing on areas such as derivatives pricing, risk management, and algorithmic trading.&rdquo;</p>
</blockquote>
<p>After spending some time with them, I have noted some key differences in the way they work and how I do it. This post describes some aspects that quantitative analysts/researchers do very well that we (ML people) should keep in mind even when not dealing with financial data.</p>
<hr>
<h2 id="simplicity-driven">Simplicity-driven</h2>
<p>Let&rsquo;s start with a quote from Nick Patterson who worked at <a href="https://en.wikipedia.org/wiki/Renaissance_Technologies">Rennaisance Technologies:</a></p>
<blockquote>
<p>&hellip;It&rsquo;s funny that I think the most important thing to do on data analysis is to do the <strong>simple things right</strong>. So, here&rsquo;s a kind of non-secret about what we did at Renaissance: in my opinion, our most important statistical tool was simple regression with one target and one independent variable. <strong>It&rsquo;s the simplest</strong> statistical model you can imagine. Any reasonably smart high school student could do it. Now we have some of the smartest people around, working in our hedge fund, we have string theorists we recruited from Harvard, and they&rsquo;re doing <strong>simple</strong> regression. Is this stupid and pointless? Should we be hiring stupider people and paying them less? And the answer is no. And the reason is nobody tells you what the variables you should be regressing [are]. What&rsquo;s the target. Should you do a nonlinear transform before you regress? What&rsquo;s the source? Should you clean your data? Do you notice when your results are obviously rubbish? And so on. And the smarter you are the less likely you are to make a stupid mistake. And that&rsquo;s why I think you often need smart people who appear to be doing something technically very easy, but actually usually not so easy. <a href="https://news.ycombinator.com/item?id=19065226">[Reference]</a></p>
</blockquote>
<p>Let&rsquo;s know continue with the wonderful post Eugene Yan wrote about <a href="https://eugeneyan.com/writing/simplicity/">&ldquo;Simplicity is An Advantage but Sadly Complexity Sells Better&rdquo;</a> where he articulates many Machine Learning examples concluding with <em>&ldquo;the objective should be to solve complex problems with as simple a solution as possible.&quot;</em></p>
<p>The sad reality is that in ML, sometimes we try to overcomplicate things just because we can. Nowadays, GPUs are largely available (even you can use them for free in something like <a href="https://colab.google/">Google Colab</a>), packages like Pytorch, Keras and Scikit-learn are extremely simple to use, and data is generally abundant (maybe you don&rsquo;t know the old <a href="https://archive.ics.uci.edu/">UCI datasets</a>, but you probably know about <a href="https://www.kaggle.com/datasets">Kaggle</a>). This has led to a bias toward always overcomplicating things from the beginning without considering any additional requirements or trade-offs.</p>
<p>In contrast, I’ve seen quants develop truly informative models with 1) a clear hypothesis (more on that later), 2) linear regression, and 3) a clean dataset.</p>
<p>Note that I am not arguing against using state-of-the-art models, but when solving a real problem, I would be more inclined to start with something simpler while considering many other aspects (cost, scalability, constraints, etc.).</p>
<h2 id="skepticalcynical-attitude">Skeptical/Cynical attitude</h2>
<p>Let’s consider two quotes in the context of evaluating (backtesting) Machine Learning models for Finance:</p>
<blockquote>
<p>&ldquo;The large amount of data allows for multiple layers of cross-validation, which minimizes the risk of overfitting. <strong>We are not so lucky in finance. Our data are limited.</strong>&quot;<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup></p>
</blockquote>
<blockquote>
<p>&ldquo;The most common mistake is being seduced by the data into thinking a model is better than it is.&quot;<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup></p>
</blockquote>
<p>I’ve seen firsthand how the nature of quants is to be skeptical. If your model seems to be doing well in most metrics, their first thought is 1) <strong>something is wrong</strong>, followed by, 2) you are <strong>overfitting</strong>.</p>
<p>When dealing with financial data, most evaluations are performed via <em>backtesting</em>, where essentially we can replay the data/experiment. The issue is that that replay <strong>is just one trajectory of a distribution</strong>. Numerous papers and books are devoted to properly using backtesting in finance. Below are a few quotes from Marcos Lopez de Prado on the importance/dangers of overfitting:</p>
<blockquote>
<p>&ldquo;&hellip; Backtest overfitting is arguably the most important open problem in all of mathematical finance. It is our equivalent to “P versus NP” in computer science.&quot;<sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup></p>
</blockquote>
<blockquote>
<p>&ldquo;&hellip; ML is a great weapon in your research arsenal, and a dangerous one to be sure.&quot;<sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup></p>
</blockquote>
<p>To me, the following paragraph summarizes the main problem of repeating the same evaluation over and over without additional considerations:</p>
<blockquote>
<p>&ldquo;One of the most pervasive mistakes in financial research is to take some data, run it through an ML algorithm, backtest the predictions, and repeat the sequence until a nice-looking backtest shows up. Academic journals are filled with such pseudo-discoveries, and even large hedge funds constantly fall into this trap. It does not matter if the backtest is a walk-forward out-of-sample. The fact that we are repeating a test over and over on the same data will likely lead to a false discovery.&quot;<sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup></p>
</blockquote>
<p>Doing ML with financial data sounds rather grim… and kind of is. Most of the time, nothing works, and when it works, it’s probably because you overfit. After all, it’s very tempting (and easy) to repeat experiments until the backtest improves because it <em>seems</em> similar to hyperparameter tuning and model validation in ML, but it is not.</p>
<h2 id="hypothesis-based">Hypothesis-based</h2>
<p>So, now the question arises: How do quants do their analysis if they don’t trust anything? Well, they do science… They follow a <strong>scientific approach</strong> based on hypothesis-experiments-evaluation. In their setting, it is common to talk about (economic or market) hypotheses that you will test.</p>
<blockquote>
<p>Establish an Ex Ante Economic Foundation: A hypothesis is developed, and the empirical tests attempt to find evidence inconsistent with the hypothesis—so-called falsifiability <sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup></p>
</blockquote>
<p>Another way of dealing with the backtest problem is to slightly change the way the analysis is done. What becomes important are the features and how they change (with respect to the hypothesis).</p>
<blockquote>
<p>Once we have found what features are important, we can learn more by conducting a number of experiments. Are these features important all the time, or only in some specific environments? What triggers a change in importance over time? Can those regime switches be predicted? Are those important features also relevant to other related financial instruments? Are they relevant to other asset classes? What are the most relevant features across all financial instruments? What is the subset of features with the highest rank correlation across the entire investment universe? This is a much better way of researching strategies than the foolish backtest cycle.<sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup></p>
</blockquote>
<p>It is so important that it is stated as the first rule of backtesting:</p>
<blockquote>
<p>MARCOS’ FIRST LAW OF BACKTESTING—IGNORE AT YOUR OWN PERIL
“Backtesting is not a research tool. Feature importance is.”<sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup></p>
</blockquote>
<hr>
<p>In summary, <strong>quants usually start with a hypothesis to test, use simple models, and are skeptical of their results.</strong> Those characteristics are no different from what ML practitioners <strong>should do</strong>; however, ML tends to work without clear hypotheses, leading to overcomplicated models and being overly enthusiastic about results.</p>
<hr>
<h3 id="something-extra">Something extra:</h3>
<p><img src="../../img/quantvsml.png" alt="Quant vs ML"></p>
<p>&ldquo;Differences between Quant and ML, from <a href="https://youtu.be/BhaJVZNpL4M?si=K5MmJqt6TJeIfbEs%22">https://youtu.be/BhaJVZNpL4M?si=K5MmJqt6TJeIfbEs&quot;</a></p>
<hr>
<p>&ndash;</p>
<section class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1" role="doc-endnote">
<p>Arnott, Robert D., Campbell R. Harvey, and Harry Markowitz. &ldquo;A backtesting protocol in the era of machine learning.&rdquo; (2018).&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2" role="doc-endnote">
<p>De Prado, Marcos Lopez. Advances in financial machine learning. John Wiley &amp; Sons, 2018.&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</section>
]]></content>
		</item>
		
		<item>
			<title>Thoughts on AI and Machine Learning for prospective/current students</title>
			<link>https://pablohl.github.io/posts/thoughts/</link>
			<pubDate>Sun, 13 Feb 2022 00:00:00 +0000</pubDate>
			
			<guid>https://pablohl.github.io/posts/thoughts/</guid>
			<description>This blog post was motivated when I was asked some interesting questions on AI and I thought it was good to share with a larger audience.
  Why did you choose AI? What excites you about it?
 I think my interest in machine learning (ML) started when I was doing my undergrad and I had to take some advanced courses. I think there were two courses &amp;ldquo;Artificial Intelligence&amp;rdquo; and &amp;ldquo;Data mining&amp;rdquo;.</description>
			<content type="html"><![CDATA[<p>This blog post was motivated when I was asked some interesting questions on AI and I thought it was good to share with a larger audience.</p>
<hr>
<blockquote>
<p>Why did you choose AI? What excites you about it?</p>
</blockquote>
<p>I think my interest in machine learning (ML) started when I was doing my undergrad and I had to take some advanced courses. I think there were two courses &ldquo;Artificial Intelligence&rdquo; and &ldquo;Data mining&rdquo;. I remember learning about decision trees, association rules, k-means and perceptrons (I also remember using <a href="https://www.cs.waikato.ac.nz/ml/weka/">Weka</a> for the first time). Of course, they were introductory courses, but, in retrospect they made a profound impression because I wanted to understand more about AI and I went to the library to do more research on the area. I was lucky to find <a href="https://www.amazon.com/LEARNING-Mcgraw-Hill-International-Mitchell-Paperback/dp/B00HRF2H6G/ref=sr_1_2?keywords=machine+learning+tom+mitchell&amp;qid=1644523232&amp;sprefix=machine+learning+tom+%2Caps%2C95&amp;sr=8-2">Tom Mitchell</a> and <a href="https://www.amazon.com/Artificial-Intelligence-A-Modern-Approach/dp/0134610997/ref=sr_1_2?crid=E8VHAICW1A87&amp;keywords=artificial+intelligence+russell+norvig&amp;qid=1644523282&amp;sprefix=artificial+intelligent+russell+norving%2Caps%2C91&amp;sr=8-2">Russell and Norving</a> books. I didn&rsquo;t really master AI or Machine learning at that time, however, it planted the idea that AI/machine learning were interesting and I wanted to learn more. That&rsquo;s kind of why I decided to continue with my Master&rsquo;s and Ph.D. in ML, because I was really curious about the potential of computers doing something intelligent (better than humans). As of today I am still intrigued by machine learning, I am amazed by <a href="https://www.technologyreview.com/2019/01/25/1436/we-analyzed-16625-papers-to-figure-out-where-ai-is-headed-next/">all the published papers</a>, <a href="https://deepmind.com/blog/article/AlphaStar-Grandmaster-level-in-StarCraft-II-using-multi-agent-reinforcement-learning">games solved</a>, and <a href="https://copilot.github.com/">use cases</a>. <strong>I find AI/ML pretty exciting and I don&rsquo;t see this trend changing in the forseeable future</strong>.</p>
<blockquote>
<p>What are some ideal skills/traits of a machine learning engineer?</p>
</blockquote>
<p>I think <strong>the first problem is defining what a machine learning engineer is</strong>. Depending on the company you might see different job titles such as <em>ML engineer, ML researcher, researcher engineer, applied scientist, research scientist</em>. Those titles do not necessarily reflect the true nature of the job. So, <strong>the way I think about machine learning engineers is a spectrum with two ingredients, software engineering and machine learning.</strong> I think the ideal machine learning engineer should master both of them, however, <strong>just being very good at one is difficult</strong>. Mastering both it&rsquo;s unusual and I think there are very few people with such a talent (FYI, if you are one of those, you&rsquo;ll probably have a great compensation).</p>
<p>Let&rsquo;s do a very quick questionnaire to justify my reasoning, let&rsquo;s assume you were given a task to develop a <a href="https://www.cortical.io/freetools/detect-language/">language identification/detection system</a>.</p>
<ul>
<li>ML1. Do you know how to model this problem as a machine learning task?</li>
<li>ML2. What is the class to be predicted and which features would you use? How would you pre-process a given dataset?</li>
<li>ML3. Which baselines would you define? Which metrics are you going to evaluate?</li>
<li>SE1. Are you able to code the above model in a modular way such that it can be read and extended by other engineers?</li>
<li>SE2. How would you determine if your model needs to be retrained? How frequently?</li>
<li>SE3. Can you define the API of your model? Can you sketch a block diagram of the training and inference processes?</li>
</ul>
<p>Do you know the answer <strong>to all</strong> the above questions? I for sure have better answers for some questions than for others. That&rsquo;s why there&rsquo;s a spectrum of knowledge and every engineer might have different strengths and weaknesses.</p>
<blockquote>
<p>Is there anything you&rsquo;d recommend a student like me who is interested in AI?</p>
</blockquote>
<p>This is a hard one, many ideas come to mind but since you are early in your career <strong>my advice would be &ldquo;don&rsquo;t give up,&quot;</strong> this has at least two interpretations.</p>
<ol>
<li>Machine learning has become a ridiculously competitive field and therefore <strong>you will face rejection at some point</strong>, it can be that a rejection from a university, a grant, a proposal, a paper, a company. It will happen and you&rsquo;ll feel miserable, but it happens to everyone. Just keep learning and trying.</li>
<li><strong>Machine learning moves incredibly fast</strong>. Years ago you could read surveys and grasp a good idea of the state of the art in a specific area. Nowadays, surveys are almost obsolete because by the time they are published dozens of new papers in the area had been pushed to ArXiv. It&rsquo;s no joke that when a paper is presented at a conference, there are already 3 or 4 improvements in workshops at the same conference. That&rsquo;s why my advice is to <strong>focus on the fundamentals</strong>. Spend time on building a solid background on the basics, those never change and you will use them many times throughout your career.</li>
</ol>
<hr>
<p>From the above it seems a bit daunting for junior students, and certainly studying AI/ML is very different to what it was years ago. However, to leave on an optimistic tone, you should also think that AI/ML offers many rewards in the long-term:</p>
<ul>
<li><a href="https://www.levels.fyi">Really good compensation packages</a></li>
<li><a href="https://deepmind.com/blog/article/alphafold-a-solution-to-a-50-year-old-grand-challenge-in-biology">The opportunity to work on complex problems</a></li>
<li><a href="https://netflixtechblog.com/artwork-personalization-c589f074ad76">Knowing that your job might reach millions of people</a></li>
</ul>
]]></content>
		</item>
		
		<item>
			<title>How to be a great supervisor (of an internship)?</title>
			<link>https://pablohl.github.io/posts/great_supervisor/</link>
			<pubDate>Mon, 13 Dec 2021 00:00:00 +0000</pubDate>
			
			<guid>https://pablohl.github.io/posts/great_supervisor/</guid>
			<description>In my previous post I discussed how to be a great intern, now I&amp;rsquo;ll try to provide a complementary view on what I think a great supervisor should do.
 Trade-offs One of the most difficult parts of being a good supervisor is deciding what role you need to play during the internship to maximize both the intern&amp;rsquo;s interests and yours. For example, as a supervisor you can &amp;ldquo;force&amp;rdquo; the intern to work on your project/ideas, but if the intern is not interested it&amp;rsquo;s going to be a huge loss of time and effort (for both).</description>
			<content type="html"><![CDATA[<p>In my previous post I discussed <a href="https://pablohl.github.io/posts/great_intern/">how to be a great intern</a>, now I&rsquo;ll try to provide a complementary view on what I think a great supervisor should do.</p>
<hr>
<h3 id="trade-offs">Trade-offs</h3>
<p>One of the most difficult parts of being a good supervisor is deciding what role you need to play during the internship <strong>to maximize both the intern&rsquo;s interests and yours</strong>. For example, as a supervisor you can &ldquo;force&rdquo; the intern to work on your project/ideas, but if the intern is not interested it&rsquo;s going to be a huge loss of time and effort (for both). The opposite is being a hands-off supervisor and giving minimal guidance to the intern. This might be a good learning experience for the intern to drive his own interests, however, the probability of completing something meaningful might be lower.
Another example of a trade-off happens when the intern &ldquo;is stuck&rdquo; (for example the intern is not sure why some experiments are giving incorrect results or the intern is not sure which experiment to try next). <strong>As a supervisor you probably have some ideas on how to solve the issue, however, you might choose not to do it right away</strong>. Why? I believe, as a supervisor <strong>you want to maximize both the outcome (and quality) of the internship project and the quality of the internship experience (i.e., how much the intern has learned).</strong> This translates into letting interns figure things out on their own in many cases. How much time to wait before actively helping the intern? That&rsquo;s the difficult part of being a good supervisor.</p>
<h3 id="focus">Focus</h3>
<p>You as a supervisor should know that <strong>internships are very short</strong>. So, try <strong>to be fully present</strong>. <strong>Do not think of other tasks you need to do. Do not casually check your email and slack. Devote the allocated time to focus on the internship project.</strong> What happens if there&rsquo;s no clear agenda on what to discuss? Then, you as a supervisor can ask more questions and lead the meeting. If there&rsquo;s nothing on the agenda that might be an indication that the intern might be stuck. Even if there&rsquo;s nothing else to discuss about the project (maybe you are waiting for some experiments to finish) I&rsquo;d say do not cancel the meeting or finish early. Use that allocated time to connect and try to build a relationship.</p>
<h3 id="motivation-and-support">Motivation and support</h3>
<p>One of the key characteristics of a good supervisor is <strong>being capable to motivate throughout the different stages of the internship</strong>. For example, at the beginning of the project a supervisor should be capable of describing some interesting problems that deserve to be studied. If the supervisor is not able to convince an intern that a problem is interesting then the intern might not want to work on that!  In contrast, at the end of the internship a supervisor should be able to describe the contribution and how the project improved previous works. This is important because when you (the intern) are close to finishing the internship you will see your work as minimal, &ldquo;not so great&rdquo; or &ldquo;just ok&rdquo;. However, probably this happens because you worked on that for some time that <em>seems</em> easy. It&rsquo;s your supervisor&rsquo;s job to convince you of the good work that you have done. Lastly, a supervisor can actively help the intern with a recommendation for a full time job. In summary, a good supervisor should support the intern at all times during the internship (and sometimes beyond that period).</p>
<h3 id="documentation-and-feedback">Documentation and feedback</h3>
<p>My Ph.D. supervisor said something like <strong>&ldquo;the better the quality of the document the better the quality of the feedback&rdquo;</strong>. What this means is that when you share a document/report that is not close to the final version (for example with some incoherent ideas, lack of structure, unclear sentences, etc) then it&rsquo;s very hard to give detailed feedback. The best one can provide is high-level comments like &ldquo;this paragraph is not clear&rdquo;, &ldquo;move this content to that section&rdquo;, &ldquo;organize the experiments in this way&rdquo;, etc. Once the document becomes easier to read and has sufficient structure then the feedback will change and it will be the point when the supervisor will make more &ldquo;interesting&rdquo; comments.</p>
<h2 id="conclusions">Conclusions</h2>
<p>I mentioned <em>a few</em> examples of what I consider makes a good supervisor. They come in different forms and people have different strengths. This means that an amazing researcher does not always equal an amazing supervisor. An amazing engineer does not always mean an amazing supervisor. I think co-supervision might be a good tactic to get better feedback as an intern and to improve/grow as a supervisor.</p>
]]></content>
		</item>
		
		<item>
			<title>How to be a great intern?</title>
			<link>https://pablohl.github.io/posts/great_intern/</link>
			<pubDate>Mon, 22 Nov 2021 00:00:00 +0000</pubDate>
			
			<guid>https://pablohl.github.io/posts/great_intern/</guid>
			<description>Tips and suggestions to make the most of an internship in tech. I’ve worked with many talented interns at throughtout my career and I wanted to share some general suggestions that might help you to improve your internship. Note: My experience is with research internships where the final outcome could be a paper submission.
 Expectations The first key aspect to understand are the expectations of your internship (both for you and your supervisor).</description>
			<content type="html"><![CDATA[<h1 id="tips-and-suggestions-to-make-the-most-of-an-internship-in-tech">Tips and suggestions to make the most of an internship in tech.</h1>
<p>I’ve worked with many talented interns at throughtout my career and I wanted to share some general suggestions that might help you to improve your internship. Note: My experience is with <em>research</em> internships where the final outcome could be a paper submission.</p>
<hr>
<h3 id="expectations">Expectations</h3>
<p>The first key aspect to understand are the <em>expectations of your internship (both for you and your supervisor)</em>. An internship <em>is not like a mini Ph.D</em>. and <em>it is not like a final project</em> in a CS course.</p>
<p>I&rsquo;ve seen internships where the supervisor has clearly delineated a project with a problem and possible solutions, then your work it&rsquo;s probably going to be following your supervisor&rsquo;s ideas. On the other hand, it could be a very open-ended internship where you are given the freedom to work on whatever you like. <em>Which one is better? Depends on you and your goals</em>. If you get an internship after 3 years on Ph.D. you probably have a good idea of how to do research and areas of interest. However, for a more junior intern, having more concrete steps and ideas would be more beneficial.</p>
<p>My suggestion is to understand the expectations of your internship and talk with your supervisor on what style/approach you are going to use on the project. Setting clear expectations from the beginning could help you to plan and have a better experience in your internship.</p>
<h3 id="duration">Duration</h3>
<p>A second highly important aspect that you need to understand is that <strong>internships are very short</strong>, 4-6 months are usually not enough to:</p>
<ol>
<li>Understand a new topic/problem</li>
<li>Propose a solution</li>
<li>Implement it</li>
<li>Run experiments</li>
<li>Write a paper</li>
</ol>
<p>Usually, <em>your supervisor</em> needs to drive and define the first two tasks (your supervisor will also help you with the paper). In contrast, <em>you will be the main driver</em> of the implementation and experimental evaluation (this means that your supervisor won&rsquo;t be checking your code).</p>
<h3 id="documentation">Documentation</h3>
<p>Another key element to understand is that supervisors usually have other projects as their main responsibility and doing <em>context switching is tiring and difficult</em>. You as an intern can help your supervisor by <strong>keeping a structured documentation</strong> of the research/experiments/ideas that you have performed (I’ve seen people use Google docs and markdown editors, like hackMD.io). You don’t want to spend too much time on the document but it needs to have a logical structure that makes it easier to find something. A simple approach is to sort chronologically (most recent ideas/experiments at the beginning), this would also be helpful later if you need to write a report of your internship.</p>
<h3 id="communication">Communication</h3>
<p>Documentation will be a starting point to have conversations and hopefully fruitful discussions. But, for this to happen you&rsquo;ll need to develop <strong>good communication with your supervisor and/or peers</strong>. Unfortunately, this will not happen on the first day. You haven&rsquo;t worked together and you don&rsquo;t know how to communicate efficiently, probably it will take some time for you to adjust and improve that. My suggestion is to build a relationship with your peers/supervisors. How to do it? well&hellip;  try to not always talk about your project or work, spend some time chitchatting. I believe it should really be the supervisor&rsquo;s work to make the effort and build a relationship but supervisors are far from perfect and they might not know how to do it, so, you can try leading this part.</p>
<h3 id="quality-of-feedback">Quality of Feedback</h3>
<p>A last suggestion is trying to <strong>maximize the quality of the feedback you get in your meetings</strong>. As I mentioned, your supervisor might not be checking every day on you, so use your time efficiently. If you have followed the suggestion above about keeping good documentation, <em>you can share it with your supervisor before the meeting</em>, they can take a look if they have time and they will already have a context to start with during the meeting. You can also be prepared for the meeting with a list of questions/ideas/experiments that you want to discuss further.</p>
<h2 id="conclusions">Conclusions</h2>
<p>In my opinion internships have a lot of <em>randomness</em> involved and usually are a hit or miss situation. I think that if you applied and got an internship position you will have amazing expectations of proposing a new algorithm, submitting a new a paper and possibly getting an offer of a job after the internship&hellip; that’s a good attitude, but you also need to accept that <em><strong>internships sometimes don’t work out and that is also fine</strong></em>. I usually tell interns that the goal that I have for the internship is that:</p>
<ul>
<li><strong>They should have learned something new</strong>: in a technical way, for example by reading and discussing a couple of papers, and at least implementing/testing one in more depth.</li>
<li><strong>They should have improved a few <em>soft skills</em></strong>: at the end of the internship you should have given a couple of presentations, prepared a report (or paper) and you had to interact and exchange ideas with other peers. These skills are valuable no matter which company or job, so practice and keep improving.</li>
</ul>
<p>If we also get a paper submission that’s extra.</p>
<p>A last piece of advice is that internships are a good way to get a tiny look into a company but don’t be fooled to think that you will understand every aspect of it. Enjoy your internship and try to learn as much as possible.</p>
]]></content>
		</item>
		
		<item>
			<title>Hello world</title>
			<link>https://pablohl.github.io/posts/hello_world/</link>
			<pubDate>Wed, 17 Nov 2021 00:00:00 +0000</pubDate>
			
			<guid>https://pablohl.github.io/posts/hello_world/</guid>
			<description>I started my blog</description>
			<content type="html"><![CDATA[<p>I started my blog</p>
]]></content>
		</item>
		
	</channel>
</rss>
